# SegmentaciÃ³n semÃ¡ntica del dataset COCO mediante  UNET y arquitecturas ensemble


A lo largo de este proyecto se aborda mediante diferentes notebooks el proceso completo de la resoluciÃ³n de un problema de **segmentaciÃ³n semÃ¡ntica** ðŸ–¼ï¸ mediante diferentes **arquitecturas** tanto **convolucionales** como con arquitecturas que usan **mecanismos de atenciÃ³n**, ademÃ¡s de su despliegue para la productivizaciÃ³n mediante una aplicaciÃ³n web ðŸŒ.
El trabajo se ha estructurado siguiendo la metodologÃ­a `CRISP-DM`, organizada de la siguiente manera:

### **ComprensiÃ³n del negocio (*Business Understanding* en CRISP_DM) y comprensiÃ³n de los datos (*Data Understanding*) ðŸ§ ðŸ“Š **
En estas dos fases se incluye tanto el entendimeinto del objetivo del proyecto como una primera carga y evaluaciÃ³n del conjunto de datos, donde se detectan patrones que influirÃ¡n en fases posteriores de la metodologÃ­a con el objetivo de la obtenciÃ³n de los mejores resultados posibles que se adecuen de forma Ã³ptima al problema definido.

- La carga inicial y el entendimiento del objetivo se desarrollan en el notebook  [`src/data_loading.ipynb`](src/data_loading.ipynb).
- El anÃ¡lisis exploratorio del conjunto de datos tiene lugar en el notebook [`src\exploratory_data_analysis.ipynb`](src\exploratory_data_analysis.ipynb).


### **PreparaciÃ³n de los datos (*Data Preparation*) ðŸ› ï¸  **
A lo largo de esta fase se incluye la fase de comprensiÃ³n de las imÃ¡genes en dimensiones comunes, la carga del dataset en un formato mÃ¡s eficiente para el entrenamiento de los modelos, en este caso `tf.tfrecord`, y el sampleamiento del conjunto de datos para disminuir el desvalanceo. AdemÃ¡s, se definirÃ¡ la fase de **data augmentation**, que permite la obtenciÃ³n de diferentes muestras a partir del conjunto de datos sampleado que forzarÃ¡n al modelo a aprender.

El contenido de esta fase se desarrolla en el notebook [`src\data_preprocessing.ipynb`](src\data_preprocessing.ipynb).

### **Modelado de los Datos (*Modeling*) ðŸ¤– **
Fase que comprende el entrenamiento de modelos que permitirÃ¡n resolver el problema definido. En este caso los modelos y arquitecturas empleadas han sido:
1) Modelo baseline, en este caso la UNET ðŸ§¬. Contenido en el notebook [`src\data_modeling_UNET.ipynb`](src\data_modeling_UNET.ipynb).
2) Arquitectura ensemble empleando YoloV8 (no fundacional, pero entrenado en el mismo conjunto de datos) + SAM ðŸ§ª . Desarrollada en el notebook [`src\data_modeling_YOLO_SAM.ipynb`](src\data_modeling_YOLO_SAM.ipynb).
3) Arquitectura ensemble fundacional, con SAM + Retinanet ðŸ§  . Implementado en [`src\data_modeling_RetinaNet_SAM.ipynb`](src\data_modeling_RetinaNet_SAM.ipynb).
4) Arquitectura ensemble inversa ðŸ”„ , donde primero se segmenta y luego se clasifica, usando SAM + CLIP. Contenida en el notebook [`src\data_modeling_SAM_CLIP.ipynb`](src\data_modeling_SAM_CLIP.ipynb).


### **EvaluaciÃ³n de los resultados (*Evaluation*) ðŸ“ˆ **

Se comparan los resultados obtenidos por cada uno de los modelos desarrollados atendiendo a diferentes criterios ðŸ“Š . 
Esta comparativa de resultados tiene lugar en el fichero [`src\results_comparative.ipynb`](src\results_comparative.ipynb).



### **ImplementaciÃ³n y productivizaciÃ³n (*Deployment*)ðŸš€ **
Fase que comprende la puesta en funcionamiento de pipelines que permiten el uso de los modelos en un entorno usable en la vida real. En este caso se ha productivizado el modelo mediante una aplicaciÃ³n web desarrollada en el framework `Dash` ðŸ’» .

Esta puede ser encontrada en el directorio [`src\deployment\src\app.py`](src\deployment\src\app.py) y al inicializarla despliega en un puerto local un aplicativo web que permite el uso de los pipeline de las arquitecturas *ensemble* implementadas de forma intiutiva.




Para la reproducciÃ³n de los expermientos realizados se facilita un fichero `pyproject.toml` que contiene todas las librerias empleadas y sus versiones correspondientes.
Para su instalaciÃ³n, se han de seguir los pasos siguientes: 
1) InstalaciÃ³n de poetry mediante el *pip*
2) EjecuciÃ³n del comando por consola `poetry install`
3) EjecuciÃ³n de los notebook. En el caso de que VSCode no seleccione el entorno creado de forma automÃ¡tica, selecciÃ³n manual del mismo mediante `CTRL + SHIT + P` -> Select Python interpreter
4) Para el despliegie de la web, ejecutar el fichero [`src\deployment\src\app.py`](src\deployment\src\app.py)




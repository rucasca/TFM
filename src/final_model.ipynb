{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data modeling y evaluation (II)\n",
    "A lo largo de este notebook se cubre parte de la cuarta y quinta fase de la metodología CRIPS-DM, en este caso el modelado y la evaluación de los resultados\n",
    "\n",
    "\n",
    "1. **Comprensión del Negocio (Business Understanding)**\n",
    "   - Consistente en el entendimiento del objetivo del proyecto.\n",
    "\n",
    "2. **Comprensión de los Datos (Data Understanding)**\n",
    "   - Relacionada con la carga y primera evaluación del conjunto de datos. Se divide a su vez en :\n",
    "\n",
    "\n",
    "3. **Preparación de los Datos (Data Preparation)** \n",
    "   - Consistente en la limpieza, preparación y extracción de características de los datos.\n",
    "\n",
    "4. <span style=\"color:#66FF99;\">**Modelado (Modeling)**  </span> \n",
    "\n",
    "\n",
    "   Relacionada con la selección del modelo y el ajuste hiperparamétrico del mismo. En este caso, se han probado dos modelos diferentes, donde en este notebook se desarrolla la implementación del primero:\n",
    "\n",
    "\n",
    "   4.1. Primer modelo baseline inicial: se hace uso de la arquitectura U-Net entrenada con el dataset en bruto.\n",
    "\n",
    "\n",
    "   <span style=\"color:#66FF99;\">**4.2. Modelo final: ensemble de modelos, que combina las salidas de Fast-SAM y CLIP con la arquitectura U-Net para resolver el problema de segmentación.**</span> \n",
    "\n",
    "5. <span style=\"color:#66FF99;\">**Evaluación (Evaluation)**</span>  \n",
    "   - Evaluación de los resultados obtenidos por el modelo.\n",
    "\n",
    "6. **Implementación (Deployment)**  \n",
    "   - Integración del modelo de forma que sea accesible para su uso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data modeling\n",
    "\n",
    "En este apartado se realiza una el entrenamiento de un primer modelo que nos servirá como baseline, que en este caso se trata de el uso de la arquitectura U-Net, adaptada al número de canales de salida para la obtención de las máscaras con cada una de las clases de salida correspondiente.\n",
    "\n",
    "\n",
    "En primera instancia es necesario la generación eficiende del pipeline que carga las imágenes de COCO y sus correspondientes máscaras en memoria. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello, se cargan las librerias a usar a lo largo del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128, 128)  # Resize images to this size\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_MODEL_FOLDER = \"my_trained_models\" # Nombre del directorio donde se almacenan los pesos de los modeos a guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_id):\n",
    "    image_info = coco.loadImgs(image_id)[0]\n",
    "    image_path = os.path.join(coco_images_dir, image_info['file_name'])\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_mask(image_id):\n",
    "    mask_path = os.path.join(coco_masks_dir, f\"{image_id}.png\")\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=1)  # Single channel mask\n",
    "    mask = tf.image.resize(mask, IMAGE_SIZE, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    mask = tf.cast(mask, tf.int32)\n",
    "    mask = tf.one_hot(mask[..., 0], NUM_CLASSES)  # One-hot encode the mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_generator():\n",
    "    for image_id in image_ids:\n",
    "        yield load_and_preprocess_image(image_id), load_and_preprocess_mask(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    coco_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], NUM_CLASSES), dtype=tf.float32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cache() \\\n",
    "                 .shuffle(buffer_size=1000) \\\n",
    "                 .batch(BATCH_SIZE) \\\n",
    "                 .prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape, num_classes):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    b1 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    b1 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(b1)\n",
    "\n",
    "    # Decoder\n",
    "    u1 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(b1)\n",
    "    u1 = tf.keras.layers.concatenate([u1, c2])\n",
    "    c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    u2 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "    u2 = tf.keras.layers.concatenate([u2, c1])\n",
    "    c4 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c4 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(c4)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=(1, 2))\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=(1, 2))\n",
    "    dice = (2. * intersection + 1e-7) / (union + 1e-7)\n",
    "    return tf.reduce_mean(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=(1, 2))\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=(1, 2)) - intersection\n",
    "    iou = (intersection + 1e-7) / (union + 1e-7)\n",
    "    return tf.reduce_mean(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "model = build_unet(input_shape, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', dice_coefficient, iou_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(dataset)\n",
    "print(f\"Loss: {evaluation[0]}, Accuracy: {evaluation[1]}, Dice Coefficient: {evaluation[2]}, IoU: {evaluation[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

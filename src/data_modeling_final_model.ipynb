{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f7a794",
   "metadata": {},
   "source": [
    "# 4 Modelado del conjunto de datos\n",
    "A lo largo de este notebook se cubre parte de la cuarta fase de la metodología CRIPS-DM, en este caso el desarrollo del modelo final. En este caso, el modelo se trata de un ensemble que combina el mejor predictor, RetinaNet, el mejor segmentador SAM, y la adición de una red variante de la UNET que corrige errores del modelo final.\n",
    "\n",
    "1. **Comprensión del Negocio (Business Understanding)**\n",
    "   - Consistente en el entendimiento del objetivo y requisitos del proyecto, traduciendo las necesidades a una definición analítica del problema y una estrategia para abordarlo.\n",
    "\n",
    "2. **Comprensión de los Datos (Data Understanding)**\n",
    "   - Relacionada con la carga y primera evaluación del conjunto de datos. Se divide a su vez en carga y análisis exploratorio del conjunto de datos.\n",
    "\n",
    "3. **Preparación de los Datos (Data Preparation)** \n",
    "   - Consistente en la limpieza, preparación y extracción de características de los datos, de vital importancia para el modelado dado que determinará la calidad de los datos.\n",
    "\n",
    "4. <span style=\"color:#66FF99;\">**Modelado (Modeling)**  </span> \n",
    "   - Relacionada con la elección del modelo de machine learning y el ajuste hiperparamétrico. En este caso, el modelo a emplear se tratará de un ensemble que combina la obtención de detecciones de objetos mediante `RetinaNet`, la obtención de las máscaras mediante `Segment Anything Model` y la corrección de errores de una variante de la `UNET`.\n",
    "\n",
    "5. **Evaluación (Evaluation)**  \n",
    "   - Evaluación de los resultados obtenidos por el modelo, determinando si cumple con los requisitos definidos en la primera de las fases y si es factible su implementación.\n",
    "\n",
    "6. **Implementación (Deployment)**  \n",
    "   - Integración del modelo de forma que sea accesible para su uso, en este caso mediante una aplicación web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1a3a7",
   "metadata": {},
   "source": [
    "### Implementación\n",
    "\n",
    "El entrenamiento de este modelo ensemble tipo stacking puede dividirse en dos bloques diferenciados, en primer lugar la pipeline de inferencia preentrenada, cuyos pesos quedan bloqueados y se limitan a generar las predicciones en formato *raw* y en segundo lado el modelo de corrección `UNET`, cuyos pesos serán entrenados desde cero.\n",
    "\n",
    "Dado que uno de los grandes blottlenecks del entrenamiento del modelo *Baseline* se ha tratado del tiempo de entrenamiento debido a la falta de recursos computacionales, para el entrenamiento del modelo se automatizará la generación del `tf.Dataset` a emplear por el segundo modelo, sin aplicar data augmentation, dado que el tiempo de inferncia medio del modelo es de 30 segundos para su primera fase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d60156",
   "metadata": {},
   "source": [
    "### 1. Pipeline de inferencia del primer bloque\n",
    "\n",
    "A continuación, se desarrolla la pipeline del primer bloque del modelo y se automatiza su carga en la estuctura de datos elegida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0105a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils \n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "from utils import  load_yaml_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from utils import plot_image_and_mask, mask_generator,plot_bounding_boxes, plot_differences_batch, plot_one_hot_encoded_masks_norm, one_hot_encoder_masks, plot_differences, get_data_id_image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "471680ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = load_yaml_file()\n",
    "objetives = yaml[\"objetive_classes\"]\n",
    "N = yaml[\"size_sample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e7c1c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=34.44s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_CLASSES = len(objetives) + 1 # Se incluye el background que estara en el canal 0\n",
    "\n",
    "DIR_TRAIN_ANNOTATIONS = yaml[\"dirs\"][\"anotaciones\"][\"train\"]\n",
    "DIR_TRAIN_IMGS = yaml[\"dirs\"][\"imagenes\"][\"train\"]\n",
    "DIR_TRAIN_IMGS = os.path.abspath(os.path.join(os.getcwd(), \"..\", DIR_TRAIN_IMGS))\n",
    "\n",
    "coco=COCO(os.path.join(os.getcwd(),\"..\", DIR_TRAIN_ANNOTATIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fcb9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = coco.loadCats(coco.getCatIds())\n",
    "category_info_id = {cat['id']: cat['name'] for cat in categories}\n",
    "category_info_name = { cat['name']:cat['id'] for cat in categories}\n",
    "\n",
    "id_objetives = [category_info_name[name] for name in objetives]\n",
    "category_info_objetive = {i:category_info_id[i] for i in id_objetives}\n",
    "category_info_objetive[0] = \"background\"\n",
    "categories_names_by_index = { i: category_info_objetive[id_cat] for i , id_cat in enumerate(sorted(category_info_objetive.keys()))}\n",
    "\n",
    "\n",
    "dict_class_index = {key:i for i , key in enumerate(sorted(category_info_objetive.keys()))}\n",
    "mapper_indexModel_index_result = {value:i for i, value in enumerate(sorted(category_info_objetive.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONS_TRHESHOLD = 0.3\n",
    "CONS_THRESHOLD_SEGMENTATION = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_ids = set(coco.getImgIds(catIds=id_objetives[0]))\n",
    "\n",
    "for cat in id_objetives[1:]:\n",
    "    img_ids |= set(coco.getImgIds(catIds=cat))\n",
    "\n",
    "img_ids = list(img_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "544fb979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sam_checkpoint = r\"C:\\Users\\ruben\\Desktop\\code_tfm\\models\\SAM\\sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device= \"cpu\" )\n",
    "\n",
    "sam_model = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16352b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetinaNet(\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-2): 3 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelP6P7(\n",
       "        (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (anchor_generator): AnchorGenerator()\n",
       "  (head): RetinaNetHead(\n",
       "    (classification_head): RetinaNetClassificationHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 819, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (regression_head): RetinaNetRegressionHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retina_model = retinanet_resnet50_fpn(pretrained=True)\n",
    "retina_model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_mask(current_scores, mask,categories_names_by_index ):\n",
    "\n",
    "    one_hot_scores = np.zeros((len(categories_names_by_index), mask.shape[0], mask.shape[1]))\n",
    "\n",
    "    for clase in sorted(categories_names_by_index.keys()):\n",
    "        # print(\"mask shapes\", mask.shape)\n",
    "        # print(\"class\", clase)\n",
    "        # print(\"shapes \", one_hot_scores.shape)\n",
    "        # print(\"scores shape \", current_scores.shape)\n",
    "        one_hot_scores[clase, :, :] = np.where(mask == clase,current_scores , 0)\n",
    "\n",
    "    one_hot_scores[0, :, :] = 1 - one_hot_scores[1:, :, :].sum(axis=0)\n",
    "\n",
    "    #print(\"output has shape\", one_hot_scores.shape)\n",
    "\n",
    "    return one_hot_scores\n",
    "\n",
    "def process_inference_retina_sam(image, retina_model, sam_model, mapper_indexModel_index_result, category_info_objetive):\n",
    "\n",
    "    # RETINANET INFERENCE\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inference = retina_model(img_tensor)\n",
    "        inference = inference[0]\n",
    "\n",
    "\n",
    "    # SAM INFERENCE\n",
    "\n",
    "    sam_model.set_image(image)\n",
    "    final_mask = np.zeros(image.shape[:2], dtype=np.int8)\n",
    "    current_scores = np.zeros(image.shape[:2], dtype=np.float32)\n",
    "\n",
    "    masks_image = []\n",
    "    scores_image = []\n",
    "    labels_image = []\n",
    "    #category_info_objetive = {v: k for k, v in categories_index_by_name.items()}\n",
    "    # print(inference)\n",
    "    # print(f\"{inference['boxes']}=\")\n",
    "\n",
    "    flag_found = 0\n",
    "    \n",
    "    for box, score, label in zip(inference['boxes'], inference['scores'], inference['labels']):\n",
    "        if(score > CONS_TRHESHOLD and label.item() in category_info_objetive.keys()):\n",
    "            label = mapper_indexModel_index_result[label.item()]\n",
    "            masks, scores, _ = sam_model.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box= np.array(box.tolist())[None,:],\n",
    "                multimask_output=False\n",
    "            )\n",
    "            if np.any(masks):\n",
    "                flag_found = 1\n",
    "                masks_image.append(masks)\n",
    "                scores_image.append(scores)\n",
    "                labels_image.append(label)\n",
    "\n",
    "                mask_values = np.where(masks, scores, 0)\n",
    "                final_mask = np.where(mask_values > current_scores , label, final_mask)\n",
    "                current_scores = np.maximum(mask_values, current_scores)\n",
    "\n",
    "    # Output preprocessing \n",
    "    if(flag_found):\n",
    "        final_mask = final_mask[0]\n",
    "    current_scores = current_scores[0]\n",
    "    final_mask = get_one_hot_mask(current_scores, final_mask,categories_names_by_index )\n",
    "\n",
    "    \n",
    "\n",
    "    return final_mask, current_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be91a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = get_data_id_image(img_ids[0], coco,DIR_TRAIN_IMGS , category_info_objetive)\n",
    "\n",
    "final_mask, current_scores = process_inference_retina_sam(image, retina_model, sam_model, mapper_indexModel_index_result, category_info_objetive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af6a1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_path_train = r\"C:\\Users\\ruben\\Desktop\\code_tfm\\src\\dataset_train_final.tfrecord\"\n",
    "tfrecord_path_test = r\"C:\\Users\\ruben\\Desktop\\code_tfm\\src\\dataset_test_final.tfrecord\"\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74b43048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redimensionar_recorte(img, mask, target_shape):\n",
    "\n",
    "    input_alt,input_anc,_= img.shape\n",
    "    target_alt,target_anc,_= target_shape\n",
    "\n",
    "    recorte= img[:min(input_alt, target_alt),:min(input_anc, target_anc), :]\n",
    "    mascara_recorte =mask[:min(input_alt, target_alt),:min(input_anc, target_anc)]\n",
    "\n",
    "    padding_top = max(target_alt-recorte.shape[0],0)\n",
    "    padding_dcha = max(target_anc-recorte.shape[1],0)\n",
    "\n",
    "    padded = np.pad(recorte,((padding_top, 0),(0,padding_dcha),(0,0)),mode='constant',constant_values=0)\n",
    "    padded_mascara = np.pad(mascara_recorte,((padding_top,0),(0,padding_dcha)),mode='constant',constant_values=0)\n",
    "\n",
    "    return padded, padded_mascara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1212d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_THRESHOLD_SIZE_CROPPING = 300\n",
    "\n",
    "def insertar_mascara_random(img, mask_orig, target_class, target_channel):\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        img_ids =  coco.getImgIds(catIds=[target_class])\n",
    "        target_id = random.choice(img_ids)\n",
    "        target_info  = coco.loadImgs(target_id)[0]\n",
    "        target_path  = os.path.join(DIR_TRAIN_IMGS,target_info['file_name'])\n",
    "        target = cv2.imread(target_path)\n",
    "        target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = coco.getAnnIds(imgIds=target_id,  catIds=[target_class],  iscrowd=False)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        ann = random.choice(anns)\n",
    "        mascara_target = coco.annToMask(ann)\n",
    "\n",
    "\n",
    "        shape = img.shape\n",
    "        target, mascara_target = redimensionar_recorte(target,mascara_target,shape)\n",
    "\n",
    "        if np.sum(mascara_target) > CONST_THRESHOLD_SIZE_CROPPING:\n",
    "            break \n",
    "\n",
    "    bool_mascara_3d = mascara_target[:,:,np.newaxis]\n",
    "    result_imagen = np.where(bool_mascara_3d,target,img)\n",
    "    mask_orig[mascara_target.astype(bool)] = target_channel\n",
    "\n",
    "\n",
    "    return result_imagen, mask_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff72963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(img_id, coco,path_images, id_objetives ):\n",
    "\n",
    "    img, mask = mask_generator(coco, img_id, ids_masks= id_objetives  ,path_images = path_images)\n",
    "    #print(\"masl\", np.unique(mask))\n",
    "    mask=np.vectorize(lambda x:dict_class_index.get(x,x))(mask)\n",
    "    img = np.array(img)\n",
    "    for _ in range(4):\n",
    "        class_cropping = random.choice(id_objetives)\n",
    "        target_channel = mapper_indexModel_index_result[class_cropping]\n",
    "        img, mask = insertar_mascara_random(img, mask, class_cropping,target_channel)\n",
    "\n",
    "\n",
    "    ###   PIPELINE INFERENCE   #####\n",
    "    _ , current_scores  = process_inference_retina_sam(image, retina_model, sam_model, mapper_indexModel_index_result, category_info_objetive)\n",
    "\n",
    "    # print(\"shae\", mask.shape)\n",
    "    # print(\"masl\", np.unique(mask))\n",
    "\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST) \n",
    "    img_codificada = tf.io.encode_jpeg(img).numpy() \n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_raw = mask.tobytes()\n",
    "    mask_shape = list(mask.shape) \n",
    "\n",
    "    pipeline_output =  current_scores.astype(np.uint8)\n",
    "    pipeline_output  = pipeline_output.tobytes()\n",
    "   \n",
    "\n",
    "    feature = {\n",
    "        \"image\":tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_codificada])),\n",
    "        \"pipeline_output\":tf.train.Feature(bytes_list=tf.train.BytesList(value=[mask_raw])),\n",
    "        \"mask\":tf.train.Feature(bytes_list=tf.train.BytesList(value=[mask_raw])),\n",
    "        \"mask_shape\":tf.train.Feature(int64_list=tf.train.Int64List(value=mask_shape))\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a744cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "total_id_images = img_ids[:batch_size*(len(img_ids)//batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_train_test = 0.95\n",
    "n_batches_train  = round((len(total_id_images)//batch_size)*proportion_train_test)\n",
    "\n",
    "#train_image_ids = random.sample(total_id_images, n_batches_train*batch_size)\n",
    "#test_image_ids = [item for item in total_id_images if item not in train_image_ids]\n",
    "\n",
    "train_image_ids = random.sample(total_id_images, 1*batch_size)\n",
    "test_image_ids = random.sample(total_id_images, 1*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando Conjunto de datos Train:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando Conjunto de datos Train: 100%|██████████| 16/16 [09:52<00:00, 37.04s/it]\n",
      "Generando Conjunto de datos Test: 100%|██████████| 16/16 [09:59<00:00, 37.49s/it]\n"
     ]
    }
   ],
   "source": [
    "with tf.io.TFRecordWriter(tfrecord_path_train) as writer:\n",
    "    for img_id in tqdm(train_image_ids, desc=\"Generando Conjunto de datos Train\"):\n",
    "        example = serialize_example(img_id,coco,DIR_TRAIN_IMGS, id_objetives)\n",
    "        writer.write(example)\n",
    "\n",
    "with tf.io.TFRecordWriter(tfrecord_path_test) as writer:\n",
    "    for img_id in tqdm(test_image_ids, desc=\"Generando Conjunto de datos Test\"):\n",
    "        example = serialize_example(img_id, coco,DIR_TRAIN_IMGS, id_objetives)\n",
    "        writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75a7a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"pipeline_output\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"mask\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"mask_shape\": tf.io.FixedLenFeature([2], tf.int64),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    img =tf.io.decode_jpeg(example[\"image\"]) \n",
    "    img = tf.image.convert_image_dtype(img,tf.float32)  \n",
    "\n",
    "    shape = example[\"mask_shape\"]\n",
    "    \n",
    "    mask =tf.io.decode_raw(example[\"mask\"],tf.uint8)\n",
    "    mask =tf.reshape(mask, shape)\n",
    "    mask =tf.one_hot(mask, depth=N_CLASSES)\n",
    "\n",
    "    mask_pipeline =tf.io.decode_raw(example[\"pipeline_output\"],tf.uint8)\n",
    "    mask_pipeline =tf.reshape(mask_pipeline, shape)\n",
    "    \n",
    "    return img,mask, mask_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec8d0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_aux(example_proto):\n",
    "\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"pipeline_output\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"mask\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"mask_shape\": tf.io.FixedLenFeature([2], tf.int64),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    img =tf.io.decode_jpeg(example[\"image\"]) \n",
    "    img = tf.image.convert_image_dtype(img,tf.float32)  \n",
    "\n",
    "    shape = example[\"mask_shape\"]\n",
    "    \n",
    "    mask =tf.io.decode_raw(example[\"mask\"],tf.uint8)\n",
    "    mask =tf.reshape(mask, shape)\n",
    "    mask =tf.one_hot(mask, depth=N_CLASSES)\n",
    "\n",
    "    mask_pipeline =tf.io.decode_raw(example[\"pipeline_output\"],tf.uint8)\n",
    "    mask_pipeline =tf.reshape(mask_pipeline, shape)\n",
    "\n",
    "    final_input = tf.concat([img, tf.cast(mask_pipeline, tf.float32)], axis=-1) \n",
    "    \n",
    "    return final_input,mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim del primer batch de Train (4, 256, 256, 3)\n",
      "Dim del primer batch de Test (4, 256, 256, 11)\n"
     ]
    }
   ],
   "source": [
    "def carga_dataset(tfrecord_path, batch_size=32, size_mezcla=100, training=True):\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "    dataset = raw_dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if training:\n",
    "        dataset=dataset.shuffle(size_mezcla)\n",
    "        dataset=dataset.repeat()\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_ds = carga_dataset(tfrecord_path_train, batch_size=4)\n",
    "\n",
    "for img_batch, mask_batch, mask_pipeline in train_ds.take(1):\n",
    "    print(\"Dim del primer batch de Train\", img_batch.shape)\n",
    "    print(\"Dim del primer batch de Train\", img_batch.shape)\n",
    "    print(\"Dim del primer batch de Test\", mask_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Paso 1: copiar contenido del dataset original al nuevo archivo\n",
    "# with tf.io.TFRecordWriter(\"nuevo_dataset.tfrecord\") as writer:\n",
    "#     for record in tf.data.TFRecordDataset(\"original.tfrecord\"):\n",
    "#         writer.write(record.numpy())\n",
    "\n",
    "#     # Paso 2: añadir nuevos ejemplos\n",
    "#     for img_id in nuevas_imagenes:\n",
    "#         example = serialize_example(img_id, coco, DIR_TRAIN_IMGS, id_objetives)\n",
    "#         writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

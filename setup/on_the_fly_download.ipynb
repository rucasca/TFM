{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You use TensorFlow DType <dtype: 'int64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\ruben\\tensorflow_datasets\\coco\\2017\\1.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a758933bea40d6830330e1c71697a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99b19b6fd8c4f29b6f28eea61f243e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62b132bffec492abe62aa0dde409722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc87fad326f47cca79fd87f0c8a52e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42230d269144f1f8ee35ff9c75602c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset, info \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoco/2017\u001b[39m\u001b[38;5;124m'\u001b[39m, with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# The dataset contains different splits like 'train', 'validation', 'test'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:661\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[0;32m    655\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[0;32m    656\u001b[0m     name,\n\u001b[0;32m    657\u001b[0m     data_dir,\n\u001b[0;32m    658\u001b[0m     builder_kwargs,\n\u001b[0;32m    659\u001b[0m     try_gcs,\n\u001b[0;32m    660\u001b[0m )\n\u001b[1;32m--> 661\u001b[0m _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    664\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:517\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[1;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    516\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 517\u001b[0m   dbuilder\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:756\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format, permissions)\u001b[0m\n\u001b[0;32m    754\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 756\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    757\u001b[0m       dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m    758\u001b[0m       download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m    759\u001b[0m   )\n\u001b[0;32m    761\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    762\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    763\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    764\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1752\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_config\u001b[38;5;241m.\u001b[39mmax_examples_per_split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1750\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m split_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_splits(dl_manager, download_config)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[0;32m   1755\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m splits_lib\u001b[38;5;241m.\u001b[39mSplitDict(split_infos)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1727\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._generate_splits\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1720\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m split_name, generator \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   1721\u001b[0m       split_generators\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1722\u001b[0m       desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating splits...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1723\u001b[0m       unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m splits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1724\u001b[0m       leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1725\u001b[0m   ):\n\u001b[0;32m   1726\u001b[0m     filename_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_filename_template(split_name\u001b[38;5;241m=\u001b[39msplit_name)\n\u001b[1;32m-> 1727\u001b[0m     future \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39msubmit_split_generation(\n\u001b[0;32m   1728\u001b[0m         split_name\u001b[38;5;241m=\u001b[39msplit_name,\n\u001b[0;32m   1729\u001b[0m         generator\u001b[38;5;241m=\u001b[39mgenerator,\n\u001b[0;32m   1730\u001b[0m         filename_template\u001b[38;5;241m=\u001b[39mfilename_template,\n\u001b[0;32m   1731\u001b[0m         disable_shuffling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdisable_shuffling,\n\u001b[0;32m   1732\u001b[0m     )\n\u001b[0;32m   1733\u001b[0m     split_info_futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;66;03m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:436\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generator, Iterable):\n\u001b[1;32m--> 436\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_from_generator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuild_kwargs)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Otherwise, beam required\u001b[39;00m\n\u001b[0;32m    438\u001b[0m   unknown_generator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    439\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid split generator value for split `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    440\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected generator or apache_beam object. Got: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    442\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:505\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m    497\u001b[0m     generator,\n\u001b[0;32m    498\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples...\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    502\u001b[0m     mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m    503\u001b[0m ):\n\u001b[0;32m    504\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mencode_example(example)\n\u001b[0;32m    506\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     utils\u001b[38;5;241m.\u001b[39mreraise(e, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to encode example:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\features\\features_dict.py:241\u001b[0m, in \u001b[0;36mFeaturesDict.encode_example\u001b[1;34m(self, example_dict)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, (feature, example_value) \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mzip_dict(\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_dict, example_dict\n\u001b[0;32m    239\u001b[0m ):\n\u001b[0;32m    240\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     example[k] \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mencode_example(example_value)\n\u001b[0;32m    242\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     utils\u001b[38;5;241m.\u001b[39mreraise(\n\u001b[0;32m    244\u001b[0m         e, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn <\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m> with name \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    245\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\features\\image_feature.py:327\u001b[0m, in \u001b[0;36mImage.encode_example\u001b[1;34m(self, image_or_path_or_fobj)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_or_path_or_fobj):\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Convert the given image into a dict convertible to tf example.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image_encoder\u001b[38;5;241m.\u001b[39mencode_image_or_path(image_or_path_or_fobj)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\tensorflow_datasets\\core\\features\\image_feature.py:113\u001b[0m, in \u001b[0;36m_ImageEncoder.encode_image_or_path\u001b[1;34m(self, image_or_path_or_fobj)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_or_path_or_fobj, epath\u001b[38;5;241m.\u001b[39mPathLikeCls):\n\u001b[0;32m    112\u001b[0m   image_or_path_or_fobj \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(image_or_path_or_fobj)\n\u001b[1;32m--> 113\u001b[0m   encoded_image \u001b[38;5;241m=\u001b[39m epath\u001b[38;5;241m.\u001b[39mPath(image_or_path_or_fobj)\u001b[38;5;241m.\u001b[39mread_bytes()\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_or_path_or_fobj, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    115\u001b[0m   encoded_image \u001b[38;5;241m=\u001b[39m image_or_path_or_fobj\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\etils\\epath\\abstract_path.py:145\u001b[0m, in \u001b[0;36mPath.read_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reads contents of self as bytes.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\etils\\epath\\gpath.py:255\u001b[0m, in \u001b[0;36m_GPath.open\u001b[1;34m(self, mode, encoding, errors, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    252\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    253\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` not supported in `open()`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    254\u001b[0m   )\n\u001b[1;32m--> 255\u001b[0m gfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path_str, mode)\n\u001b[0;32m    256\u001b[0m gfile \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mcast(typing\u001b[38;5;241m.\u001b[39mIO[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m]], gfile)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gfile\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\etils\\epath\\backend.py:133\u001b[0m, in \u001b[0;36m_OsPathBackend.open\u001b[1;34m(self, path, mode)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m   encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, mode, encoding\u001b[38;5;241m=\u001b[39mencoding)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load('coco/2017', with_info=True)\n",
    "\n",
    "# The dataset contains different splits like 'train', 'validation', 'test'\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "# Inspect the dataset info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target class id (e.g., 1 for 'person')\n",
    "target_class_id = 1\n",
    "\n",
    "# Function to filter images containing the target class\n",
    "def filter_class(instance):\n",
    "    objects = instance['objects']\n",
    "    return any(obj['label'] == target_class_id for obj in objects)\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_train_ds = train_ds.filter(filter_class)\n",
    "\n",
    "# Preprocess the dataset to create images and masks\n",
    "def preprocess(instance):\n",
    "    image = instance['image']  # The input image\n",
    "    objects = instance['objects']  # Annotations for objects\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)  # Initialize mask with zeros\n",
    "\n",
    "    # Set the mask pixel values corresponding to the target classId\n",
    "    for obj in objects:\n",
    "        if obj['label'] == target_class_id:\n",
    "            # Convert bounding box to mask\n",
    "            bbox = obj['bbox']  # Format: [ymin, xmin, ymax, xmax]\n",
    "            mask[int(bbox[0]):int(bbox[2]), int(bbox[1]):int(bbox[3])] = target_class_id\n",
    "\n",
    "    # Normalize the image (optional, depending on your model's needs)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "train_ds = filtered_train_ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Batch and shuffle the dataset\n",
    "train_ds = train_ds.shuffle(1000).batch(8).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def unet(input_size=(256, 256, 3)):\n",
    "    inputs = layers.Input(input_size)\n",
    "\n",
    "    # Contracting path (Encoder)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Expanding path (Decoder)\n",
    "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    # Output layer with softmax activation for segmentation\n",
    "    output = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[output])\n",
    "    return model\n",
    "\n",
    "# Instantiate the U-Net model\n",
    "model = unet(input_size=(256, 256, 3))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the U-Net model\n",
    "model.fit(train_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
